# vLLM 自定义算子映射表（第 1 阶段）

[English](vllm-custom-ops-mapping-table.md) | [简体中文](vllm-custom-ops-mapping-table.zh-CN.md)

范围：先覆盖 `torch.ops._C_cache_ops`（KV cache 写入/读取/量化相关）。
这些算子位于 GPT 推理的热路径：K/V 写入分页缓存，随后在注意力中使用
$A = \text{softmax}(QK^T/\sqrt{d})V$ 进行计算。

## 映射表：`torch.ops._C_cache_ops.*`

| 算子 | Python 调用点 + GPT 推理用途 | 原生实现 + 说明/伪代码 |
|---|---|---|
| `swap_blocks` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2457) 封装；用于 KV offload 中在 CPU/GPU 之间交换块 [vllm/vllm/v1/kv_offload/worker/cpu_gpu.py](vllm/vllm/v1/kv_offload/worker/cpu_gpu.py#L172)。用途：KV 块迁移/淘汰。 | 实现： [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L33)。按 block_mapping 列表逐块 memcpy（CPU 或 GPU）。伪代码：`for (src_block,dst_block) in block_mapping: memcpy(dst+dst_block*bytes, src+src_block*bytes)`。
| `reshape_and_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2374) 封装；分页注意力 KV 写入 [vllm/vllm/v1/attention/ops/paged_attn.py](vllm/vllm/v1/attention/ops/paged_attn.py#L42)。用途：prefill/decode 时把 K/V 写入分页 KV cache。 | 实现：kernel + wrapper 在 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L142) 和 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L581)。按 `slot_mapping` 把 token 写入 cache，使用打包布局并可选 FP8 缩放。伪代码：`slot=slot_mapping[t]; if slot<0: continue; block=slot//B; off=slot%B; write key/value into cache`。
| `reshape_and_cache_flash` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2396) 封装；被多个后端使用：FlashAttention [vllm/vllm/v1/attention/backends/flash_attn.py](vllm/vllm/v1/attention/backends/flash_attn.py#L793)、FlashInfer [vllm/vllm/v1/attention/backends/flashinfer.py](vllm/vllm/v1/attention/backends/flashinfer.py#L1343)、Flex [vllm/vllm/v1/attention/backends/flex_attention.py](vllm/vllm/v1/attention/backends/flex_attention.py#L911)、Tree [vllm/vllm/v1/attention/backends/tree_attn.py](vllm/vllm/v1/attention/backends/tree_attn.py#L374)。用途：在 NHD/HND 布局下写 KV；以 `slot_mapping` 的长度决定有效 token。 | 实现：kernel + wrapper 在 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L202) 和 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L626)。支持连续 heads 或逐 head 布局，并支持逐 head FP8 scale。伪代码：`slot=slot_mapping[t]; if slot<0: continue; dst=cache[block,off]; 向量化拷贝 K/V（可缩放）`。
| `concat_and_cache_mla` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2418) 封装；MLA 注意力写入 joint cache [vllm/vllm/model_executor/layers/attention/mla_attention.py](vllm/vllm/model_executor/layers/attention/mla_attention.py#L524)。用途：把 NoPE `kv_c` 与 RoPE `k_pe` 拼接写入 MLA KV cache。 | 实现：kernel + wrapper 在 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L290) 和 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L698)。把 `kv_c` 和 `k_pe` 拼成一条 cache entry，可选 FP8 缩放。伪代码：`dst=kv_cache[block,off]; dst[:kv_lora]=kv_c; dst[kv_lora:]=k_pe`。
| `concat_and_cache_mla_rope_fused` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2431) 封装；MLA 的 RoPE 融合写入路径（启用时）。用途：在一个 kernel 中对 Q/K 做 RoPE 并写入 KV cache，减少带宽。 | 实现： [vllm/csrc/cache_kernels_fused.cu](vllm/csrc/cache_kernels_fused.cu#L204)。先对 `q_pe`/`k_pe` 做 RoPE，再写入 RoPE K 与 NoPE `kv_c`（可选 FP8 缩放）。伪代码：`apply_rope(q_pe,k_pe); slot=slot_mapping[t]; write k_pe, kv_c into kv_cache`。
| `convert_fp8` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2486) 封装；用于 FP8 工具函数 [vllm/vllm/utils/torch_utils.py](vllm/vllm/utils/torch_utils.py#L235)。用途：在 FP8 与 FP16/BF16/FP32 间转换缓存/测试张量。 | 实现： [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L762) 与 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L782)。逐元素按 scale 转换，主要用于测试/工具。伪代码：`for each block: dst[i]=fp8_scaled_convert(src[i], scale)`。
| `gather_and_maybe_dequant_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2492) 封装；用于 MLA 的分块 prefill 工作区聚合 [vllm/vllm/model_executor/layers/attention/mla_attention.py](vllm/vllm/model_executor/layers/attention/mla_attention.py#L2232)。用途：按 `block_table` 把 cache 读到 workspace，并在需要时解量化。 | 实现： [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L839) 与 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L939)。按 block_table/cu_seq_lens/token_to_seq 聚合 token，并可从 FP8 解量化。伪代码：`for token_id: seq=token_to_seq; block=block_table[seq][slot]; dst[token]=dequant(src_cache[block,off])`。
| `cp_gather_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2516) 封装；用于 MLA DCP prefill [vllm/vllm/model_executor/layers/attention/mla_attention.py](vllm/vllm/model_executor/layers/attention/mla_attention.py#L2305)。用途：为每个 batch 聚合 cache，支持 `seq_starts` 偏移。 | 实现：kernel + wrapper 在 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1072) 与 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1149)。按 batch 分块并拷贝 token。伪代码：`for batch: for token range: copy src_cache[block,offset] -> dst[seq_start+token]`。
| `cp_gather_and_upconvert_fp8_kv_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2529) 封装；用于 FlashMLA sparse prefill [vllm/vllm/v1/attention/backends/mla/flashmla_sparse.py](vllm/vllm/v1/attention/backends/mla/flashmla_sparse.py#L842)。用途：从 FP8 cache 读出并上转换到 BF16 workspace。 | 实现：kernel + wrapper 在 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L996) 与 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1210)。从 FP8 反量化 NoPE 部分并拷贝 RoPE 部分。伪代码：`for token: load fp8 bytes + scales; dequant to bf16; copy rope part`。
| `indexer_k_quant_and_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2552) 封装；用于稀疏注意力 indexer [vllm/vllm/model_executor/layers/sparse_attn_indexer.py](vllm/vllm/model_executor/layers/sparse_attn_indexer.py#L76)。用途：把 K 量化后写入 cache，并保存 block scale 用于稀疏 top-k。 | 实现：kernel + wrapper 在 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L437) 与 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1262)。计算 amax/scale 并写入量化 K 与 scale。伪代码：`amax=max(|K|); scale=f(amax); kv_cache=quant(K,scale); store scale`。
| `cp_gather_indexer_k_quant_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2564) 封装；用于稀疏注意力 indexer [vllm/vllm/model_executor/layers/sparse_attn_indexer.py](vllm/vllm/model_executor/layers/sparse_attn_indexer.py#L97)。用途：把量化 K 与 scales 聚合到连续缓冲区以便 top-k 评分。 | 实现：kernel + wrapper 在 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L501) 与 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1305)。用 block_table/cu_seq_lens 聚合量化 K 与 scale。伪代码：`for token: copy quantized K and scale from cache into dst buffers`。

## 备注

- 所有 `_C_cache_ops` 在 [vllm/csrc/torch_bindings.cpp](vllm/csrc/torch_bindings.cpp#L684) 注册。
- 部分平台使用 Triton 版本的 cache 写入，本表聚焦 CUDA/HIP 原生实现。
