# vLLM 自定义算子映射表（第 1 阶段）

[English](vllm-custom-ops-mapping-table.md) | [简体中文](vllm-custom-ops-mapping-table.zh-CN.md)

范围：先覆盖 `torch.ops._C_cache_ops`（KV cache 写入/读取/量化相关）。
这些算子位于 GPT 推理的热路径：K/V 写入分页缓存，随后在注意力中使用
$A = \text{softmax}(QK^T/\sqrt{d})V$ 进行计算。

## 映射表：`torch.ops._C_cache_ops.*`

| 算子 | Python 调用点 + GPT 推理用途 | 原生实现 + 说明/伪代码 |
|---|---|---|
| `swap_blocks` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2457) 封装；用于 KV offload 中在 CPU/GPU 之间交换块 [vllm/vllm/v1/kv_offload/worker/cpu_gpu.py](vllm/vllm/v1/kv_offload/worker/cpu_gpu.py#L172)。用途：KV 块迁移/淘汰。 | 实现： [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L33)。按 block_mapping 列表逐块 memcpy（CPU 或 GPU）。伪代码：`for (src_block,dst_block) in block_mapping: memcpy(dst+dst_block*bytes, src+src_block*bytes)`。
| `reshape_and_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2374) 封装；分页注意力 KV 写入 [vllm/vllm/v1/attention/ops/paged_attn.py](vllm/vllm/v1/attention/ops/paged_attn.py#L42)。用途：prefill/decode 时把 K/V 写入分页 KV cache。 | 实现：kernel + wrapper 在 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L142) 和 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L581)。按 `slot_mapping` 把 token 写入 cache，使用打包布局并可选 FP8 缩放。伪代码：`slot=slot_mapping[t]; if slot<0: continue; block=slot//B; off=slot%B; write key/value into cache`。
| `reshape_and_cache_flash` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2396) 封装；被多个后端使用：FlashAttention [vllm/vllm/v1/attention/backends/flash_attn.py](vllm/vllm/v1/attention/backends/flash_attn.py#L793)、FlashInfer [vllm/vllm/v1/attention/backends/flashinfer.py](vllm/vllm/v1/attention/backends/flashinfer.py#L1343)、Flex [vllm/vllm/v1/attention/backends/flex_attention.py](vllm/vllm/v1/attention/backends/flex_attention.py#L911)、Tree [vllm/vllm/v1/attention/backends/tree_attn.py](vllm/vllm/v1/attention/backends/tree_attn.py#L374)。用途：在 NHD/HND 布局下写 KV；以 `slot_mapping` 的长度决定有效 token。 | 实现：kernel + wrapper 在 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L202) 和 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L626)。支持连续 heads 或逐 head 布局，并支持逐 head FP8 scale。伪代码：`slot=slot_mapping[t]; if slot<0: continue; dst=cache[block,off]; 向量化拷贝 K/V（可缩放）`。
| `concat_and_cache_mla` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2418) 封装；MLA 注意力写入 joint cache [vllm/vllm/model_executor/layers/attention/mla_attention.py](vllm/vllm/model_executor/layers/attention/mla_attention.py#L524)。用途：把 NoPE `kv_c` 与 RoPE `k_pe` 拼接写入 MLA KV cache。 | 实现：kernel + wrapper 在 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L290) 和 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L698)。把 `kv_c` 和 `k_pe` 拼成一条 cache entry，可选 FP8 缩放。伪代码：`dst=kv_cache[block,off]; dst[:kv_lora]=kv_c; dst[kv_lora:]=k_pe`。
| `concat_and_cache_mla_rope_fused` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2431) 封装；MLA 的 RoPE 融合写入路径（启用时）。用途：在一个 kernel 中对 Q/K 做 RoPE 并写入 KV cache，减少带宽。 | 实现： [vllm/csrc/cache_kernels_fused.cu](vllm/csrc/cache_kernels_fused.cu#L204)。先对 `q_pe`/`k_pe` 做 RoPE，再写入 RoPE K 与 NoPE `kv_c`（可选 FP8 缩放）。伪代码：`apply_rope(q_pe,k_pe); slot=slot_mapping[t]; write k_pe, kv_c into kv_cache`。
| `convert_fp8` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2486) 封装；用于 FP8 工具函数 [vllm/vllm/utils/torch_utils.py](vllm/vllm/utils/torch_utils.py#L235)。用途：在 FP8 与 FP16/BF16/FP32 间转换缓存/测试张量。 | 实现： [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L762) 与 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L782)。逐元素按 scale 转换，主要用于测试/工具。伪代码：`for each block: dst[i]=fp8_scaled_convert(src[i], scale)`。
| `gather_and_maybe_dequant_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2492) 封装；用于 MLA 的分块 prefill 工作区聚合 [vllm/vllm/model_executor/layers/attention/mla_attention.py](vllm/vllm/model_executor/layers/attention/mla_attention.py#L2232)。用途：按 `block_table` 把 cache 读到 workspace，并在需要时解量化。 | 实现： [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L839) 与 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L939)。按 block_table/cu_seq_lens/token_to_seq 聚合 token，并可从 FP8 解量化。伪代码：`for token_id: seq=token_to_seq; block=block_table[seq][slot]; dst[token]=dequant(src_cache[block,off])`。
| `cp_gather_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2516) 封装；用于 MLA DCP prefill [vllm/vllm/model_executor/layers/attention/mla_attention.py](vllm/vllm/model_executor/layers/attention/mla_attention.py#L2305)。用途：为每个 batch 聚合 cache，支持 `seq_starts` 偏移。 | 实现：kernel + wrapper 在 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1072) 与 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1149)。按 batch 分块并拷贝 token。伪代码：`for batch: for token range: copy src_cache[block,offset] -> dst[seq_start+token]`。
| `cp_gather_and_upconvert_fp8_kv_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2529) 封装；用于 FlashMLA sparse prefill [vllm/vllm/v1/attention/backends/mla/flashmla_sparse.py](vllm/vllm/v1/attention/backends/mla/flashmla_sparse.py#L842)。用途：从 FP8 cache 读出并上转换到 BF16 workspace。 | 实现：kernel + wrapper 在 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L996) 与 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1210)。从 FP8 反量化 NoPE 部分并拷贝 RoPE 部分。伪代码：`for token: load fp8 bytes + scales; dequant to bf16; copy rope part`。
| `indexer_k_quant_and_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2552) 封装；用于稀疏注意力 indexer [vllm/vllm/model_executor/layers/sparse_attn_indexer.py](vllm/vllm/model_executor/layers/sparse_attn_indexer.py#L76)。用途：把 K 量化后写入 cache，并保存 block scale 用于稀疏 top-k。 | 实现：kernel + wrapper 在 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L437) 与 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1262)。计算 amax/scale 并写入量化 K 与 scale。伪代码：`amax=max(|K|); scale=f(amax); kv_cache=quant(K,scale); store scale`。
| `cp_gather_indexer_k_quant_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2564) 封装；用于稀疏注意力 indexer [vllm/vllm/model_executor/layers/sparse_attn_indexer.py](vllm/vllm/model_executor/layers/sparse_attn_indexer.py#L97)。用途：把量化 K 与 scales 聚合到连续缓冲区以便 top-k 评分。 | 实现：kernel + wrapper 在 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L501) 与 [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1305)。用 block_table/cu_seq_lens 聚合量化 K 与 scale。伪代码：`for token: copy quantized K and scale from cache into dst buffers`。

## 备注

- 所有 `_C_cache_ops` 在 [vllm/csrc/torch_bindings.cpp](vllm/csrc/torch_bindings.cpp#L684) 注册。
- 部分平台使用 Triton 版本的 cache 写入，本表聚焦 CUDA/HIP 原生实现。

## 映射表：`torch.ops._C` 注意力算子（paged attention + merge）

| 算子 | Python 调用点 + GPT 推理用途 | 原生实现 + 说明/伪代码 |
|---|---|---|
| `paged_attention_v1` | 封装见 [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L33)。用途：在分页 KV cache 上做 decode 注意力，按 $A = \text{softmax}(QK^T/\sqrt{d})V$ 计算，依赖 block_table 与 seq_lens。 | 实现： [vllm/csrc/attention/paged_attention_v1.cu](vllm/csrc/attention/paged_attention_v1.cu#L20) 调度到核心 kernel [vllm/csrc/attention/attention_kernels.cuh](vllm/csrc/attention/attention_kernels.cuh#L41)。支持 head_size/block_size 特化与 block-sparse 参数。伪代码：`对每个 seq/head 遍历 blocks; 计算 QK 与 softmax; 累加 V`。
| `paged_attention_v2` | 封装见 [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L77)。用途：分块（partition）softmax 的 paged attention，先做分片，再归并。 | 实现： [vllm/csrc/attention/paged_attention_v2.cu](vllm/csrc/attention/paged_attention_v2.cu#L20) 启动分片 kernel + reduce kernel；核心数学在 [vllm/csrc/attention/attention_kernels.cuh](vllm/csrc/attention/attention_kernels.cuh#L41)。伪代码：`每个 partition 计算局部 softmax 与 logits; 记录 exp_sums/max_logits; reduce 得到最终输出`。
| `merge_attn_states` | 封装见 [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L187)，通过调度器 [vllm/vllm/v1/attention/ops/merge_attn_states.py](vllm/vllm/v1/attention/ops/merge_attn_states.py#L9) 合并部分注意力结果（prefix/suffix 或 split-KV）。用途：用 log-sum-exp 稳定合并输出。 | 实现：Triton 版本在 [vllm/vllm/v1/attention/ops/triton_merge_attn_states.py](vllm/vllm/v1/attention/ops/triton_merge_attn_states.py#L11)（参考 arXiv:2501.01005 §2.2）。若 CUDA 支持，则调用 `torch.ops._C.merge_attn_states` 的自定义扩展实现。伪代码：`max_lse=max(p_lse,s_lse); out=(p_out*exp(p_lse-max_lse)+s_out*exp(s_lse-max_lse))/(exp(p_lse-max_lse)+exp(s_lse-max_lse))`。

## 映射表：归一化 + RoPE + 激活

| 算子 | Python 调用点 + GPT 推理用途 | 原生实现 + 说明/伪代码 |
|---|---|---|
| `rms_norm` | 封装见 [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L332)；RMSNorm 层调用 [vllm/vllm/model_executor/layers/layernorm.py](vllm/vllm/model_executor/layers/layernorm.py#L18-L31)。用途：Transformer 归一化，$y = x / \sqrt{\text{mean}(x^2)+\epsilon} * w$。 | CUDA 内核在 [vllm/csrc/layernorm_kernels.cu](vllm/csrc/layernorm_kernels.cu)。先做方差归约，再缩放并乘权重；对 FP16/BF16 做向量化。伪代码：`var=mean(x^2); y = x * rsqrt(var+eps) * w`。
| `fused_add_rms_norm` | 封装见 [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py)；用于残差 + RMSNorm 融合路径 [vllm/vllm/model_executor/layers/layernorm.py](vllm/vllm/model_executor/layers/layernorm.py)。用途：把残差相加与 RMSNorm 合并为一次内存遍历。 | CUDA 内核在 [vllm/csrc/layernorm_kernels.cu](vllm/csrc/layernorm_kernels.cu)（`fused_add_rms_norm_kernel`）。先计算 $x \leftarrow x + r$，再执行 RMSNorm。伪代码：`x = x + r; var=mean(x^2); x = x * rsqrt(var+eps) * w`。
| `rotary_embedding` | 封装见 [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L318)；RoPE 前向调用 [vllm/vllm/model_executor/layers/rotary_embedding/base.py](vllm/vllm/model_executor/layers/rotary_embedding/base.py#L200-L225)。用途：对 Q/K 应用 RoPE，使用缓存的 $\cos$/$\sin$ 旋转前 `rot_dim`。 | CUDA 内核在 [vllm/csrc/pos_encoding_kernels.cu](vllm/csrc/pos_encoding_kernels.cu)。支持 GPT‑NeoX 或 GPT‑J 风格旋转，原地更新 Q/K。伪代码：`x' = x*cos - y*sin; y' = y*cos + x*sin`。
| `silu_and_mul` | SwiGLU 激活使用 [vllm/vllm/model_executor/layers/activation.py](vllm/vllm/model_executor/layers/activation.py#L115-L150)。用途：MLP gating，$\text{silu}(x)=x\sigma(x)$，输出 $\text{silu}(x_1) * x_2$。 | CUDA 内核在 [vllm/csrc/activation_kernels.cu](vllm/csrc/activation_kernels.cu)。对齐时使用 128-bit 向量化加载；计算 silu 后相乘（或 `mul_and_silu` 反向顺序）。伪代码：`out[i] = silu(x[i]) * y[i]`。
| `mul_and_silu` | SwiGLU 变体 [vllm/vllm/model_executor/layers/activation.py](vllm/vllm/model_executor/layers/activation.py)。用途：$x_1 * \text{silu}(x_2)$（顺序相反）用于门控 MLP。 | CUDA 内核在 [vllm/csrc/activation_kernels.cu](vllm/csrc/activation_kernels.cu)。与 `silu_and_mul` 同族内核，仅调整激活顺序。伪代码：`out[i] = x[i] * silu(y[i])`。
| `gelu_and_mul` | GeGLU 激活 [vllm/vllm/model_executor/layers/activation.py](vllm/vllm/model_executor/layers/activation.py)。用途：$\text{GELU}(x_1) * x_2$。 | CUDA 内核在 [vllm/csrc/activation_kernels.cu](vllm/csrc/activation_kernels.cu)（`gelu_and_mul`）。使用精确 GELU（erf）后相乘。伪代码：`out[i] = gelu(x[i]) * y[i]`。
| `gelu_tanh_and_mul` | GeGLU 的 tanh 近似 [vllm/vllm/model_executor/layers/activation.py](vllm/vllm/model_executor/layers/activation.py)。用途：$\text{GELU}_\tanh(x_1) * x_2$（近似加速）。 | CUDA 内核在 [vllm/csrc/activation_kernels.cu](vllm/csrc/activation_kernels.cu)（`gelu_tanh_and_mul`）。伪代码：`out[i] = gelu_tanh(x[i]) * y[i]`。
| `fatrelu_and_mul` | FATReLU 门控 [vllm/vllm/model_executor/layers/activation.py](vllm/vllm/model_executor/layers/activation.py)。用途：先对 gate 做阈值 ReLU，再与另一半相乘，$\max(x_1, t) * x_2$。 | CUDA 内核在 [vllm/csrc/activation_kernels.cu](vllm/csrc/activation_kernels.cu)（`act_and_mul_kernel_with_param` + `fatrelu_kernel`）。伪代码：`out[i] = max(x[i], threshold) * y[i]`。
| `swigluoai_and_mul` | GPT‑OSS 的 SwiGLU 变体 [vllm/vllm/model_executor/layers/activation.py](vllm/vllm/model_executor/layers/activation.py)。用途：带 clamp 的 OpenAI 风格 SwiGLU（alpha/limit）。 | CUDA 内核在 [vllm/csrc/activation_kernels.cu](vllm/csrc/activation_kernels.cu)（`swigluoai_and_mul_kernel`）。伪代码：`gate=clamp(gate); up=clamp(up); out=(up+1) * gate * sigmoid(gate*alpha)`。
