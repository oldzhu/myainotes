# vLLM custom ops mapping table (phase 1)

[English](vllm-custom-ops-mapping-table.md) | [Chinese (ZH-CN)](vllm-custom-ops-mapping-table.zh-CN.md)

Scope: first pass on `torch.ops._C_cache_ops` (KV cache write/read/quant ops).
These ops sit on the hot path for GPT inference: K/V are written to a paged cache
and later gathered for attention $A = \text{softmax}(QK^T/\sqrt{d})V$.

## Mapping table: `torch.ops._C_cache_ops.*`

| Op | Python call site(s) + GPT inference role | Native implementation + notes/pseudo code |
|---|---|---|
| `swap_blocks` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2457) wrapper; used in KV offload to swap cache blocks CPU<->GPU [vllm/vllm/v1/kv_offload/worker/cpu_gpu.py](vllm/vllm/v1/kv_offload/worker/cpu_gpu.py#L172). Role: move KV blocks during offload/eviction. | Implementation: [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L33). Copies block-sized chunks between src/dst with a block mapping list (CPU or GPU). Pseudocode: `for (src_block,dst_block) in block_mapping: memcpy(dst+dst_block*bytes, src+src_block*bytes)`.
| `reshape_and_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2374) wrapper; used by paged attention KV writes [vllm/vllm/v1/attention/ops/paged_attn.py](vllm/vllm/v1/attention/ops/paged_attn.py#L42). Role: write freshly computed K/V into paged KV cache for prefill/decode. | Implementation: kernel + wrapper in [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L142) and [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L581). Writes per-token K/V into block cache using `slot_mapping`, with packed layout and optional FP8 scaling. Pseudocode: `slot=slot_mapping[t]; if slot<0: continue; block=slot//B; off=slot%B; write key/value into cache at (block,off)`.
| `reshape_and_cache_flash` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2396) wrapper; used by multiple attention backends: FlashAttention [vllm/vllm/v1/attention/backends/flash_attn.py](vllm/vllm/v1/attention/backends/flash_attn.py#L793), FlashInfer [vllm/vllm/v1/attention/backends/flashinfer.py](vllm/vllm/v1/attention/backends/flashinfer.py#L1343), Flex [vllm/vllm/v1/attention/backends/flex_attention.py](vllm/vllm/v1/attention/backends/flex_attention.py#L911), Tree [vllm/vllm/v1/attention/backends/tree_attn.py](vllm/vllm/v1/attention/backends/tree_attn.py#L374). Role: write K/V into cache in NHD or HND layout; uses `slot_mapping` length to skip padding. | Implementation: kernel + wrapper in [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L202) and [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L626). Handles contiguous or per-head layouts and optional per-head FP8 scales. Pseudocode: `slot=slot_mapping[t]; if slot<0: continue; compute dst ptr via block/offset; vectorized copy key/value (with scale)`.
| `concat_and_cache_mla` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2418) wrapper; used in MLA attention to write NoPE + RoPE parts into joint cache [vllm/vllm/model_executor/layers/attention/mla_attention.py](vllm/vllm/model_executor/layers/attention/mla_attention.py#L524). Role: build MLA KV cache entries from latent `kv_c` + RoPE `k_pe`. | Implementation: kernel + wrapper in [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L290) and [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L698). Concatenates `kv_c` and `k_pe` into per-token cache entry; optional FP8 scaling. Pseudocode: `dst = kv_cache[block,off]; dst[0:kv_lora_rank]=kv_c; dst[kv_lora_rank:]=k_pe`.
| `concat_and_cache_mla_rope_fused` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2431) wrapper; fused RoPE + cache write for MLA (used when enabled). Role: rotate Q/K with RoPE and write K + latent KV to cache in one kernel to reduce bandwidth. | Implementation: [vllm/csrc/cache_kernels_fused.cu](vllm/csrc/cache_kernels_fused.cu#L204). Applies RoPE to `q_pe` and `k_pe` in-place, then writes both RoPE K and NoPE `kv_c` into cache (with optional FP8 scaling). Pseudocode: `apply_rope(q_pe,k_pe); slot=slot_mapping[t]; write k_pe, kv_c into kv_cache`.
| `convert_fp8` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2486) wrapper; used by fp8 utility to generate fp8 tensors [vllm/vllm/utils/torch_utils.py](vllm/vllm/utils/torch_utils.py#L235). Role: convert cache or test tensors between FP8 and FP16/BF16/FP32. | Implementation: [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L762) and [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L782). Kernel converts elementwise with scale; used mostly for testing/utilities. Pseudocode: `for each block: dst[i]=fp8_scaled_convert(src[i], scale)`.
| `gather_and_maybe_dequant_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2492) wrapper; used in MLA chunked prefill to gather cache into workspace [vllm/vllm/model_executor/layers/attention/mla_attention.py](vllm/vllm/model_executor/layers/attention/mla_attention.py#L2232). Role: read KV blocks by `block_table` and dequantize into contiguous workspace for attention. | Implementation: [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L839) and wrapper [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L939). Gathers tokens via block_table/cu_seq_lens/token_to_seq, optionally dequantizes FP8. Pseudocode: `for token_id: find seq via token_to_seq; block=block_table[seq][slot]; dst[token]=dequant(src_cache[block,offset])`.
| `cp_gather_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2516) wrapper; used for context-parallel (DCP) MLA prefill [vllm/vllm/model_executor/layers/attention/mla_attention.py](vllm/vllm/model_executor/layers/attention/mla_attention.py#L2305). Role: gather cache for each batch into workspace, supporting `seq_starts` offsets. | Implementation: kernel + wrapper in [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1072) and [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1149). Splits work across blocks for each batch; copies tokens by block table. Pseudocode: `for batch: for token range: copy src_cache[block,offset] -> dst[seq_start+token]`.
| `cp_gather_and_upconvert_fp8_kv_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2529) wrapper; used in FlashMLA sparse prefill [vllm/vllm/v1/attention/backends/mla/flashmla_sparse.py](vllm/vllm/v1/attention/backends/mla/flashmla_sparse.py#L842). Role: gather FP8 cache and upconvert to BF16 workspace for MLA attention. | Implementation: kernel + wrapper in [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L996) and [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1210). Dequantizes FP8 (NoPE) and copies RoPE bytes into BF16 workspace. Pseudocode: `for token: load fp8 bytes + scales; dequant to bf16; copy rope part`.
| `indexer_k_quant_and_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2552) wrapper; used in sparse attention indexer prefill/decode [vllm/vllm/model_executor/layers/sparse_attn_indexer.py](vllm/vllm/model_executor/layers/sparse_attn_indexer.py#L76). Role: quantize K into FP8-like cache with per-block scales for sparse top-k indexing. | Implementation: kernel + wrapper in [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L437) and [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1262). Computes per-block scale (amax) and stores quantized K + scales. Pseudocode: `amax = max(|K|); scale = f(amax); kv_cache[token]=quant(K,scale); store scale`.
| `cp_gather_indexer_k_quant_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2564) wrapper; used in sparse attention indexer [vllm/vllm/model_executor/layers/sparse_attn_indexer.py](vllm/vllm/model_executor/layers/sparse_attn_indexer.py#L97). Role: gather quantized K + scales into contiguous buffers for top-k scoring. | Implementation: kernel + wrapper in [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L501) and [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1305). Gathers per-token quantized K and scales using block table and cu_seq_lens. Pseudocode: `for token: copy quantized K and scale from cache into dst buffers`.

## Notes

- All `_C_cache_ops` are registered in [vllm/csrc/torch_bindings.cpp](vllm/csrc/torch_bindings.cpp#L684).
- Some paths use Triton alternatives for cache write on specific platforms; this table documents the native CUDA/HIP ops.

## Mapping table: `torch.ops._C` attention ops (paged attention + merge)

| Op | Python call site(s) + GPT inference role | Native implementation + notes/pseudo code |
|---|---|---|
| `paged_attention_v1` | Wrapper in [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L33). Role: decode-time attention over paged KV cache blocks using $A = \text{softmax}(QK^T/\sqrt{d})V$ with block tables and sequence lengths. | Implementation: [vllm/csrc/attention/paged_attention_v1.cu](vllm/csrc/attention/paged_attention_v1.cu#L20) dispatches into the kernel in [vllm/csrc/attention/attention_kernels.cuh](vllm/csrc/attention/attention_kernels.cuh#L41). Supports head-size and block-size specializations and optional block-sparse parameters. Pseudocode: `for each seq/head: iterate blocks via block_tables; compute QK and softmax in shared memory; accumulate V`.
| `paged_attention_v2` | Wrapper in [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L77). Role: paged attention with partitioned softmax (split-KV) to reduce memory, then reduce across partitions. | Implementation: [vllm/csrc/attention/paged_attention_v2.cu](vllm/csrc/attention/paged_attention_v2.cu#L20) launches a partitioned kernel plus a reduce kernel; core math in [vllm/csrc/attention/attention_kernels.cuh](vllm/csrc/attention/attention_kernels.cuh#L41). Pseudocode: `for partition: compute partial softmax/logits; store exp_sums + max_logits; reduce partitions to final output`.
| `merge_attn_states` | Wrapper in [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L187); used via the dispatcher in [vllm/vllm/v1/attention/ops/merge_attn_states.py](vllm/vllm/v1/attention/ops/merge_attn_states.py#L9) to combine partial attention outputs (prefix/suffix or split-KV). Role: numerically stable merge of attention outputs using log-sum-exp. | Implementation: Triton fallback in [vllm/vllm/v1/attention/ops/triton_merge_attn_states.py](vllm/vllm/v1/attention/ops/triton_merge_attn_states.py#L11) (references arXiv:2501.01005 ยง2.2). When CUDA supports it, a custom C++ op is used via `torch.ops._C.merge_attn_states` (source in extension). Pseudocode: `max_lse = max(p_lse, s_lse); out = (p_out*exp(p_lse-max_lse) + s_out*exp(s_lse-max_lse)) / (exp(p_lse-max_lse)+exp(s_lse-max_lse))`.

## Mapping table: normalization + RoPE + activation

| Op | Python call site(s) + GPT inference role | Native implementation + notes/pseudo code |
|---|---|---|
| `rms_norm` | Wrapper in [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L332); used by RMSNorm layer [vllm/vllm/model_executor/layers/layernorm.py](vllm/vllm/model_executor/layers/layernorm.py#L18-L31). Role: normalization in transformer blocks, $y = x / \sqrt{\text{mean}(x^2)+\epsilon} * w$. | CUDA kernel in [vllm/csrc/layernorm_kernels.cu](vllm/csrc/layernorm_kernels.cu). Computes variance via reduction, then scales and applies weight; vectorized for FP16/BF16. Pseudocode: `var=mean(x^2); y = x * rsqrt(var+eps) * w`.
| `rotary_embedding` | Wrapper in [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L318); called in rotary embedding forward [vllm/vllm/model_executor/layers/rotary_embedding/base.py](vllm/vllm/model_executor/layers/rotary_embedding/base.py#L200-L225). Role: apply RoPE to Q/K before attention, rotating pairs with $\cos$/$\sin$ from cache. | CUDA kernel in [vllm/csrc/pos_encoding_kernels.cu](vllm/csrc/pos_encoding_kernels.cu). Applies GPT-NeoX or GPT-J style rotation to the first `rot_dim` of Q/K, in-place. Pseudocode: `x' = x*cos - y*sin; y' = y*cos + x*sin` per pair.
| `silu_and_mul` | Used in SwiGLU activation [vllm/vllm/model_executor/layers/activation.py](vllm/vllm/model_executor/layers/activation.py#L115-L150). Role: MLP gating $\text{silu}(x) = x\sigma(x)$ then multiply with the other half: $\text{silu}(x_1) * x_2$. | CUDA kernel in [vllm/csrc/activation_kernels.cu](vllm/csrc/activation_kernels.cu). Vectorized 128-bit loads when aligned; computes silu and multiplies (or reverse for `mul_and_silu`). Pseudocode: `out[i] = silu(x[i]) * y[i]`.
| `fused_add_rms_norm` | Wrapper in [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py); used in fused residual+norm path [vllm/vllm/model_executor/layers/layernorm.py](vllm/vllm/model_executor/layers/layernorm.py). Role: combine residual add + RMSNorm in one pass to reduce memory traffic. | CUDA kernel in [vllm/csrc/layernorm_kernels.cu](vllm/csrc/layernorm_kernels.cu) (`fused_add_rms_norm_kernel`). Computes $x \leftarrow x + r$, then applies RMSNorm in-place with vectorized path when aligned. Pseudocode: `x = x + r; var=mean(x^2); x = x * rsqrt(var+eps) * w`.
| `mul_and_silu` | Used in SwiGLU variant [vllm/vllm/model_executor/layers/activation.py](vllm/vllm/model_executor/layers/activation.py). Role: $x_1 * \text{silu}(x_2)$ (reversed order) for gated MLP blocks. | CUDA kernel in [vllm/csrc/activation_kernels.cu](vllm/csrc/activation_kernels.cu). Same kernel family as `silu_and_mul` with reversed activation order. Pseudocode: `out[i] = x[i] * silu(y[i])`.
| `gelu_and_mul` | Used in GeGLU activation [vllm/vllm/model_executor/layers/activation.py](vllm/vllm/model_executor/layers/activation.py). Role: $\text{GELU}(x_1) * x_2$ for gated MLP blocks. | CUDA kernel in [vllm/csrc/activation_kernels.cu](vllm/csrc/activation_kernels.cu) (`gelu_and_mul`). Uses exact GELU (erf) then multiply. Pseudocode: `out[i] = gelu(x[i]) * y[i]`.
| `gelu_tanh_and_mul` | Used in GeGLU with tanh approximation [vllm/vllm/model_executor/layers/activation.py](vllm/vllm/model_executor/layers/activation.py). Role: $\text{GELU}_\tanh(x_1) * x_2$ (approximate) for performance. | CUDA kernel in [vllm/csrc/activation_kernels.cu](vllm/csrc/activation_kernels.cu) (`gelu_tanh_and_mul`). Pseudocode: `out[i] = gelu_tanh(x[i]) * y[i]`.
| `fatrelu_and_mul` | Used in FATReLU gating [vllm/vllm/model_executor/layers/activation.py](vllm/vllm/model_executor/layers/activation.py). Role: apply thresholded ReLU on gate then multiply with the other half, $\max(x_1, t) * x_2$. | CUDA kernel in [vllm/csrc/activation_kernels.cu](vllm/csrc/activation_kernels.cu) (`act_and_mul_kernel_with_param` + `fatrelu_kernel`). Pseudocode: `out[i] = max(x[i], threshold) * y[i]`.
| `swigluoai_and_mul` | Used in GPT-OSS SwiGLU variant [vllm/vllm/model_executor/layers/activation.py](vllm/vllm/model_executor/layers/activation.py). Role: clamped SwiGLU with OpenAI-style params (alpha/limit). | CUDA kernel in [vllm/csrc/activation_kernels.cu](vllm/csrc/activation_kernels.cu) (`swigluoai_and_mul_kernel`). Pseudocode: `gate=clamp(gate); up=clamp(up); out=(up+1) * gate * sigmoid(gate*alpha)`.
