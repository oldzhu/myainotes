# vLLM custom ops mapping table: attention stage

[English](vllm-custom-ops-mapping-attention.md) | [Chinese (ZH-CN)](vllm-custom-ops-mapping-attention.zh-CN.md)

Scope: `torch.ops._C` attention ops (paged attention + merge) plus sparse attention
indexing utilities, used during decode, prefill, and partitioned attention merging.

## Mapping table: `torch.ops._C` attention ops (paged attention + merge)

| Op | Python call site(s) + GPT inference role | Native implementation + notes/pseudo code |
|---|---|---|
| `paged_attention_v1` | Wrapper in [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L33). Role: decode-time attention over paged KV cache blocks using $A = \text{softmax}(QK^T/\sqrt{d})V$ with block tables and sequence lengths. | Implementation: [vllm/csrc/attention/paged_attention_v1.cu](vllm/csrc/attention/paged_attention_v1.cu#L20) dispatches into the kernel in [vllm/csrc/attention/attention_kernels.cuh](vllm/csrc/attention/attention_kernels.cuh#L41). Supports head-size and block-size specializations and optional block-sparse parameters. Pseudocode: `for each seq/head: iterate blocks via block_tables; compute QK and softmax in shared memory; accumulate V`.
| `paged_attention_v2` | Wrapper in [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L77). Role: paged attention with partitioned softmax (split-KV) to reduce memory, then reduce across partitions. | Implementation: [vllm/csrc/attention/paged_attention_v2.cu](vllm/csrc/attention/paged_attention_v2.cu#L20) launches a partitioned kernel plus a reduce kernel; core math in [vllm/csrc/attention/attention_kernels.cuh](vllm/csrc/attention/attention_kernels.cuh#L41). Pseudocode: `for partition: compute partial softmax/logits; store exp_sums + max_logits; reduce partitions to final output`.
| `merge_attn_states` | Wrapper in [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L187); used via the dispatcher in [vllm/vllm/v1/attention/ops/merge_attn_states.py](vllm/vllm/v1/attention/ops/merge_attn_states.py#L9) to combine partial attention outputs (prefix/suffix or split-KV). Role: numerically stable merge of attention outputs using log-sum-exp. | Implementation: Triton fallback in [vllm/vllm/v1/attention/ops/triton_merge_attn_states.py](vllm/vllm/v1/attention/ops/triton_merge_attn_states.py#L11) (references arXiv:2501.01005 ยง2.2). When CUDA supports it, a custom C++ op is used via `torch.ops._C.merge_attn_states` (source in extension). Pseudocode: `max_lse = max(p_lse, s_lse); out = (p_out*exp(p_lse-max_lse) + s_out*exp(s_lse-max_lse)) / (exp(p_lse-max_lse)+exp(s_lse-max_lse))`.
| `convert_vertical_slash_indexes` | Wrapper in [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L200). Role: convert vertical/slash sparse-attention indices (used by MInference-style block-sparse attention) into block-sparse lookup tables. | CUDA kernel in [vllm/csrc/attention/vertical_slash_index.cu](vllm/csrc/attention/vertical_slash_index.cu#L184). Converts 64x64 tiles into compact block indices; based on Algorithm 4 of arXiv:2407.02490. Pseudocode: `for each block: map (row,col) to block-table indices`.
| `convert_vertical_slash_indexes_mergehead` | Wrapper in [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L256). Role: merge-head variant of vertical/slash index conversion (shared indices across heads). | CUDA kernel in [vllm/csrc/attention/vertical_slash_index.cu](vllm/csrc/attention/vertical_slash_index.cu#L372). Same mapping as above, with head-merged layout. Pseudocode: `for each block: map (row,col) to merged-head block indices`.
| `top_k_per_row_prefill` | Called by sparse indexer in [vllm/vllm/model_executor/layers/sparse_attn_indexer.py](vllm/vllm/model_executor/layers/sparse_attn_indexer.py#L118). Role: select top-k attention blocks per row during prefill to build sparse attention patterns. | CUDA kernel in [vllm/csrc/sampler.cu](vllm/csrc/sampler.cu#L701). Computes per-row top-k indices over logits/importance scores. Pseudocode: `for each row: partial sort -> top_k`.
| `top_k_per_row_decode` | Called by sparse indexer in [vllm/vllm/model_executor/layers/sparse_attn_indexer.py](vllm/vllm/model_executor/layers/sparse_attn_indexer.py#L167). Role: select top-k blocks per row during decode for sparse attention. | CUDA kernel in [vllm/csrc/sampler.cu](vllm/csrc/sampler.cu#L646). Same top-k kernel tuned for decode-time shapes. Pseudocode: `for each row: partial sort -> top_k`.
