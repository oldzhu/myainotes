# vLLM custom ops mapping table: normalization + RoPE + activation stage

[English](vllm-custom-ops-mapping-norm-activation.md) | [Chinese (ZH-CN)](vllm-custom-ops-mapping-norm-activation.zh-CN.md)

Scope: normalization, positional encoding, and activation custom ops used inside
transformer blocks.

## Mapping table: normalization + RoPE + activation

| Op | Python call site(s) + GPT inference role | Native implementation + notes/pseudo code |
|---|---|---|
| `rms_norm` | Wrapper in [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L332); used by RMSNorm layer [vllm/vllm/model_executor/layers/layernorm.py](vllm/vllm/model_executor/layers/layernorm.py#L18-L31). Role: normalization in transformer blocks, $y = x / \sqrt{\text{mean}(x^2)+\epsilon} * w$. | CUDA kernel in [vllm/csrc/layernorm_kernels.cu](vllm/csrc/layernorm_kernels.cu). Computes variance via reduction, then scales and applies weight; vectorized for FP16/BF16. Pseudocode: `var=mean(x^2); y = x * rsqrt(var+eps) * w`.
| `rotary_embedding` | Wrapper in [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L318); called in rotary embedding forward [vllm/vllm/model_executor/layers/rotary_embedding/base.py](vllm/vllm/model_executor/layers/rotary_embedding/base.py#L200-L225). Role: apply RoPE to Q/K before attention, rotating pairs with $\cos$/$\sin$ from cache. | CUDA kernel in [vllm/csrc/pos_encoding_kernels.cu](vllm/csrc/pos_encoding_kernels.cu). Applies GPT-NeoX or GPT-J style rotation to the first `rot_dim` of Q/K, in-place. Pseudocode: `x' = x*cos - y*sin; y' = y*cos + x*sin` per pair.
| `fused_qk_norm_rope` | Wrapper in [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L344); fused via [vllm/vllm/compilation/qk_norm_rope_fusion.py](vllm/vllm/compilation/qk_norm_rope_fusion.py#L25) when available. Role: fuse Q/K RMSNorm + RoPE into a single kernel to reduce memory traffic before attention. | CUDA kernel in [vllm/csrc/fused_qknorm_rope_kernel.cu](vllm/csrc/fused_qknorm_rope_kernel.cu#L365). Validates head dims, applies RMSNorm to Q/K, then applies RoPE using cached $\cos$/$\sin$. Pseudocode: `q = rmsnorm(q); k = rmsnorm(k); apply_rope(q,k)`.
| `silu_and_mul` | Used in SwiGLU activation [vllm/vllm/model_executor/layers/activation.py](vllm/vllm/model_executor/layers/activation.py#L115-L150). Role: MLP gating $\text{silu}(x) = x\sigma(x)$ then multiply with the other half: $\text{silu}(x_1) * x_2$. | CUDA kernel in [vllm/csrc/activation_kernels.cu](vllm/csrc/activation_kernels.cu). Vectorized 128-bit loads when aligned; computes silu and multiplies (or reverse for `mul_and_silu`). Pseudocode: `out[i] = silu(x[i]) * y[i]`.
| `fused_add_rms_norm` | Wrapper in [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py); used in fused residual+norm path [vllm/vllm/model_executor/layers/layernorm.py](vllm/vllm/model_executor/layers/layernorm.py). Role: combine residual add + RMSNorm in one pass to reduce memory traffic. | CUDA kernel in [vllm/csrc/layernorm_kernels.cu](vllm/csrc/layernorm_kernels.cu) (`fused_add_rms_norm_kernel`). Computes $x \leftarrow x + r$, then applies RMSNorm in-place with vectorized path when aligned. Pseudocode: `x = x + r; var=mean(x^2); x = x * rsqrt(var+eps) * w`.
| `mul_and_silu` | Used in SwiGLU variant [vllm/vllm/model_executor/layers/activation.py](vllm/vllm/model_executor/layers/activation.py). Role: $x_1 * \text{silu}(x_2)$ (reversed order) for gated MLP blocks. | CUDA kernel in [vllm/csrc/activation_kernels.cu](vllm/csrc/activation_kernels.cu). Same kernel family as `silu_and_mul` with reversed activation order. Pseudocode: `out[i] = x[i] * silu(y[i])`.
| `gelu_and_mul` | Used in GeGLU activation [vllm/vllm/model_executor/layers/activation.py](vllm/vllm/model_executor/layers/activation.py). Role: $\text{GELU}(x_1) * x_2$ for gated MLP blocks. | CUDA kernel in [vllm/csrc/activation_kernels.cu](vllm/csrc/activation_kernels.cu) (`gelu_and_mul`). Uses exact GELU (erf) then multiply. Pseudocode: `out[i] = gelu(x[i]) * y[i]`.
| `gelu_tanh_and_mul` | Used in GeGLU with tanh approximation [vllm/vllm/model_executor/layers/activation.py](vllm/vllm/model_executor/layers/activation.py). Role: $\text{GELU}_\tanh(x_1) * x_2$ (approximate) for performance. | CUDA kernel in [vllm/csrc/activation_kernels.cu](vllm/csrc/activation_kernels.cu) (`gelu_tanh_and_mul`). Pseudocode: `out[i] = gelu_tanh(x[i]) * y[i]`.
| `fatrelu_and_mul` | Used in FATReLU gating [vllm/vllm/model_executor/layers/activation.py](vllm/vllm/model_executor/layers/activation.py). Role: apply thresholded ReLU on gate then multiply with the other half, $\max(x_1, t) * x_2$. | CUDA kernel in [vllm/csrc/activation_kernels.cu](vllm/csrc/activation_kernels.cu) (`act_and_mul_kernel_with_param` + `fatrelu_kernel`). Pseudocode: `out[i] = max(x[i], threshold) * y[i]`.
| `swigluoai_and_mul` | Used in GPT-OSS SwiGLU variant [vllm/vllm/model_executor/layers/activation.py](vllm/vllm/model_executor/layers/activation.py). Role: clamped SwiGLU with OpenAI-style params (alpha/limit). | CUDA kernel in [vllm/csrc/activation_kernels.cu](vllm/csrc/activation_kernels.cu) (`swigluoai_and_mul_kernel`). Pseudocode: `gate=clamp(gate); up=clamp(up); out=(up+1) * gate * sigmoid(gate*alpha)`.
