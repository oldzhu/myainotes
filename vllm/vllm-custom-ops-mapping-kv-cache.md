# vLLM custom ops mapping table: KV cache stage

[English](vllm-custom-ops-mapping-kv-cache.md) | [Chinese (ZH-CN)](vllm-custom-ops-mapping-kv-cache.zh-CN.md)

Scope: `torch.ops._C_cache_ops` (KV cache write/read/quant ops).
These ops sit on the hot path for GPT inference: K/V are written to a paged cache
and later gathered for attention $A = \text{softmax}(QK^T/\sqrt{d})V$.

## Mapping table: `torch.ops._C_cache_ops.*`

| Op | Python call site(s) + GPT inference role | Native implementation + notes/pseudo code |
|---|---|---|
| `swap_blocks` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2457) wrapper; used in KV offload to swap cache blocks CPU<->GPU [vllm/vllm/v1/kv_offload/worker/cpu_gpu.py](vllm/vllm/v1/kv_offload/worker/cpu_gpu.py#L172). Role: move KV blocks during offload/eviction. | Implementation: [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L33). Copies block-sized chunks between src/dst with a block mapping list (CPU or GPU). Pseudocode: `for (src_block,dst_block) in block_mapping: memcpy(dst+dst_block*bytes, src+src_block*bytes)`.
| `reshape_and_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2374) wrapper; used by paged attention KV writes [vllm/vllm/v1/attention/ops/paged_attn.py](vllm/vllm/v1/attention/ops/paged_attn.py#L42). Role: write freshly computed K/V into paged KV cache for prefill/decode. | Implementation: kernel + wrapper in [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L142) and [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L581). Writes per-token K/V into block cache using `slot_mapping`, with packed layout and optional FP8 scaling. Pseudocode: `slot=slot_mapping[t]; if slot<0: continue; block=slot//B; off=slot%B; write key/value into cache at (block,off)`.
| `reshape_and_cache_flash` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2396) wrapper; used by multiple attention backends: FlashAttention [vllm/vllm/v1/attention/backends/flash_attn.py](vllm/vllm/v1/attention/backends/flash_attn.py#L793), FlashInfer [vllm/vllm/v1/attention/backends/flashinfer.py](vllm/vllm/v1/attention/backends/flashinfer.py#L1343), Flex [vllm/vllm/v1/attention/backends/flex_attention.py](vllm/vllm/v1/attention/backends/flex_attention.py#L911), Tree [vllm/vllm/v1/attention/backends/tree_attn.py](vllm/vllm/v1/attention/backends/tree_attn.py#L374). Role: write K/V into cache in NHD or HND layout; uses `slot_mapping` length to skip padding. | Implementation: kernel + wrapper in [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L202) and [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L626). Handles contiguous or per-head layouts and optional per-head FP8 scales. Pseudocode: `slot=slot_mapping[t]; if slot<0: continue; compute dst ptr via block/offset; vectorized copy key/value (with scale)`.
| `concat_and_cache_mla` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2418) wrapper; used in MLA attention to write NoPE + RoPE parts into joint cache [vllm/vllm/model_executor/layers/attention/mla_attention.py](vllm/vllm/model_executor/layers/attention/mla_attention.py#L524). Role: build MLA KV cache entries from latent `kv_c` + RoPE `k_pe`. | Implementation: kernel + wrapper in [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L290) and [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L698). Concatenates `kv_c` and `k_pe` into per-token cache entry; optional FP8 scaling. Pseudocode: `dst = kv_cache[block,off]; dst[0:kv_lora_rank]=kv_c; dst[kv_lora_rank:]=k_pe`.
| `concat_and_cache_mla_rope_fused` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2431) wrapper; fused RoPE + cache write for MLA (used when enabled). Role: rotate Q/K with RoPE and write K + latent KV to cache in one kernel to reduce bandwidth. | Implementation: [vllm/csrc/cache_kernels_fused.cu](vllm/csrc/cache_kernels_fused.cu#L204). Applies RoPE to `q_pe` and `k_pe` in-place, then writes both RoPE K and NoPE `kv_c` into cache (with optional FP8 scaling). Pseudocode: `apply_rope(q_pe,k_pe); slot=slot_mapping[t]; write k_pe, kv_c into kv_cache`.
| `convert_fp8` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2486) wrapper; used by fp8 utility to generate fp8 tensors [vllm/vllm/utils/torch_utils.py](vllm/vllm/utils/torch_utils.py#L235). Role: convert cache or test tensors between FP8 and FP16/BF16/FP32. | Implementation: [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L762) and [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L782). Kernel converts elementwise with scale; used mostly for testing/utilities. Pseudocode: `for each block: dst[i]=fp8_scaled_convert(src[i], scale)`.
| `gather_and_maybe_dequant_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2492) wrapper; used in MLA chunked prefill to gather cache into workspace [vllm/vllm/model_executor/layers/attention/mla_attention.py](vllm/vllm/model_executor/layers/attention/mla_attention.py#L2232). Role: read KV blocks by `block_table` and dequantize into contiguous workspace for attention. | Implementation: [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L839) and wrapper [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L939). Gathers tokens via block_table/cu_seq_lens/token_to_seq, optionally dequantizes FP8. Pseudocode: `for token_id: find seq via token_to_seq; block=block_table[seq][slot]; dst[token]=dequant(src_cache[block,offset])`.
| `cp_gather_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2516) wrapper; used for context-parallel (DCP) MLA prefill [vllm/vllm/model_executor/layers/attention/mla_attention.py](vllm/vllm/model_executor/layers/attention/mla_attention.py#L2305). Role: gather cache for each batch into workspace, supporting `seq_starts` offsets. | Implementation: kernel + wrapper in [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1072) and [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1149). Splits work across blocks for each batch; copies tokens by block table. Pseudocode: `for batch: for token range: copy src_cache[block,offset] -> dst[seq_start+token]`.
| `cp_gather_and_upconvert_fp8_kv_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2529) wrapper; used in FlashMLA sparse prefill [vllm/vllm/v1/attention/backends/mla/flashmla_sparse.py](vllm/vllm/v1/attention/backends/mla/flashmla_sparse.py#L842). Role: gather FP8 cache and upconvert to BF16 workspace for MLA attention. | Implementation: kernel + wrapper in [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L996) and [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1210). Dequantizes FP8 (NoPE) and copies RoPE bytes into BF16 workspace. Pseudocode: `for token: load fp8 bytes + scales; dequant to bf16; copy rope part`.
| `indexer_k_quant_and_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2552) wrapper; used in sparse attention indexer prefill/decode [vllm/vllm/model_executor/layers/sparse_attn_indexer.py](vllm/vllm/model_executor/layers/sparse_attn_indexer.py#L76). Role: quantize K into FP8-like cache with per-block scales for sparse top-k indexing. | Implementation: kernel + wrapper in [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L437) and [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1262). Computes per-block scale (amax) and stores quantized K + scales. Pseudocode: `amax = max(|K|); scale = f(amax); kv_cache[token]=quant(K,scale); store scale`.
| `cp_gather_indexer_k_quant_cache` | [vllm/vllm/_custom_ops.py](vllm/vllm/_custom_ops.py#L2564) wrapper; used in sparse attention indexer [vllm/vllm/model_executor/layers/sparse_attn_indexer.py](vllm/vllm/model_executor/layers/sparse_attn_indexer.py#L97). Role: gather quantized K + scales into contiguous buffers for top-k scoring. | Implementation: kernel + wrapper in [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L501) and [vllm/csrc/cache_kernels.cu](vllm/csrc/cache_kernels.cu#L1305). Gathers per-token quantized K and scales using block table and cu_seq_lens. Pseudocode: `for token: copy quantized K and scale from cache into dst buffers`.

## Notes

- All `_C_cache_ops` are registered in [vllm/csrc/torch_bindings.cpp](vllm/csrc/torch_bindings.cpp#L684).
- Some paths use Triton alternatives for cache write on specific platforms; this table documents the native CUDA/HIP ops.
