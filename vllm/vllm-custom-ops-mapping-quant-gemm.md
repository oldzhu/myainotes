# vLLM custom ops mapping table: quantization + GEMM stage

[English](vllm-custom-ops-mapping-quant-gemm.md) | [Chinese (ZH-CN)](vllm-custom-ops-mapping-quant-gemm.zh-CN.md)

Scope: quantization and low-precision GEMM ops used in transformer linear layers.

## Mapping table: quantization + GEMM ops

| Op | Python call site(s) + GPT inference role | Native implementation + notes/pseudo code |
|---|---|---|
| `awq_dequantize` | Called in AWQ path [vllm/vllm/model_executor/layers/quantization/awq.py](vllm/vllm/model_executor/layers/quantization/awq.py#L272). Role: dequantize AWQ weights into dense blocks for GEMM or fallback paths. | CUDA kernel in [vllm/csrc/quantization/awq/gemm_kernels.cu](vllm/csrc/quantization/awq/gemm_kernels.cu#L413). Converts packed 4-bit weights + scales/zeros into FP16/BF16. Pseudocode: `for each block: dequantize(qweight, scale, zero)`.
| `awq_gemm` | Called in AWQ GEMM path [vllm/vllm/model_executor/layers/quantization/awq.py](vllm/vllm/model_executor/layers/quantization/awq.py#L275). Role: W4A16 AWQ GEMM for MLP/attention linear layers. | CUDA kernel in [vllm/csrc/quantization/awq/gemm_kernels.cu](vllm/csrc/quantization/awq/gemm_kernels.cu#L469). Fuses dequant + GEMM on packed weights. Pseudocode: `dequantize tiles; matmul`.
| `gptq_shuffle` | Called during GPTQ weight prep [vllm/vllm/model_executor/layers/quantization/gptq.py](vllm/vllm/model_executor/layers/quantization/gptq.py#L368). Role: permute packed GPTQ weights by group index before GEMM. | CUDA kernel in [vllm/csrc/quantization/gptq/q_gemm.cu](vllm/csrc/quantization/gptq/q_gemm.cu#L1853). Reorders packed bits according to `g_idx`. Pseudocode: `q_weight = permute(q_weight, g_idx)`.
| `gptq_gemm` | Called in GPTQ linear [vllm/vllm/model_executor/layers/quantization/gptq.py](vllm/vllm/model_executor/layers/quantization/gptq.py#L381). Role: 4/8-bit GPTQ GEMM with groupwise scales and zero-points. | CUDA kernel in [vllm/csrc/quantization/gptq/q_gemm.cu](vllm/csrc/quantization/gptq/q_gemm.cu#L1828). Dequantizes per group and performs GEMM. Pseudocode: `for each group: dequantize; matmul`.
| `cutlass_scaled_mm_azp` | Called in CUTLASS scaled MM path [vllm/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py](vllm/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py#L130). Role: W8A8 GEMM with activation zero-point adjustment. | CUTLASS entry in [vllm/csrc/quantization/w8a8/cutlass/scaled_mm_entry.cu](vllm/csrc/quantization/w8a8/cutlass/scaled_mm_entry.cu#L347). Dispatches by SM version and applies AZP adjustment. Pseudocode: `C = (A-azp) * B * scales + bias`.
| `cutlass_scaled_mm` | Called in CUTLASS scaled MM path [vllm/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py](vllm/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py#L140). Role: W8A8 scaled GEMM for FP8/BF16 pipelines. | CUTLASS entry in [vllm/csrc/quantization/w8a8/cutlass/scaled_mm_entry.cu](vllm/csrc/quantization/w8a8/cutlass/scaled_mm_entry.cu#L175). Dispatches to SM-specific kernels. Pseudocode: `C = (A * a_scale) * (B * b_scale)`.
| `per_token_group_fp8_quant` | Called during FP8 quantization [vllm/vllm/model_executor/layers/quantization/utils/fp8_utils.py](vllm/vllm/model_executor/layers/quantization/utils/fp8_utils.py#L926). Role: per-token-group FP8 quantization of activations. | CUDA kernel in [vllm/csrc/quantization/w8a8/fp8/per_token_group_quant.cu](vllm/csrc/quantization/w8a8/fp8/per_token_group_quant.cu#L379). Computes group scales and quantizes to FP8. Pseudocode: `scale = max(abs(x_group)); q = clamp(x/scale)`.
| `per_token_group_fp8_quant_packed` | Called during FP8 quantization [vllm/vllm/model_executor/layers/quantization/utils/fp8_utils.py](vllm/vllm/model_executor/layers/quantization/utils/fp8_utils.py#L1034). Role: pack per-token-group quant output into 8-bit/FP8 packed layout. | CUDA kernel in [vllm/csrc/quantization/w8a8/fp8/per_token_group_quant.cu](vllm/csrc/quantization/w8a8/fp8/per_token_group_quant.cu#L297). Produces packed output for downstream GEMM. Pseudocode: `quantize -> pack`.
| `scaled_fp4_quant` | Called in NVFP4 path [vllm/vllm/model_executor/layers/quantization/utils/nvfp4_utils.py](vllm/vllm/model_executor/layers/quantization/utils/nvfp4_utils.py#L214). Role: scale-and-quantize activations into FP4 for FP4 GEMM. | CUDA kernel in [vllm/csrc/quantization/fp4/nvfp4_quant_entry.cu](vllm/csrc/quantization/fp4/nvfp4_quant_entry.cu#L54). Quantizes inputs with block scales. Pseudocode: `scale = max(abs(x_block)); q = quantize_fp4(x/scale)`.
| `marlin_gemm` | Called in Marlin path [vllm/vllm/model_executor/layers/quantization/utils/marlin_utils.py](vllm/vllm/model_executor/layers/quantization/utils/marlin_utils.py#L566). Role: Marlin W4A16 GEMM for fast quantized linear layers. | CUDA kernel in [vllm/csrc/quantization/marlin/marlin.cu](vllm/csrc/quantization/marlin/marlin.cu#L531). Uses Marlin tiles and dequant inside the kernel. Pseudocode: `dequantize tiles; matmul`.
